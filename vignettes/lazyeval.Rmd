---
title: "Non-standard evaluation"
author: "Hadley Wickham"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lazy evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(lazyeval)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

This document describes lazyeval, a package that provides principled tools to perform non-standard evaluation (NSE) in R. You should read this vignette if you want to program with packages like dplyr and ggplot2[^1], or you want a principled way of working with delayed expressions in your own package. As the name suggests, non-standard evaluation breaks away from the standard evaluation (SE) rules in order to do something special. There are three common uses of NSE:

1.  __Labelling__ enhances plots and tables by using the expressions
    supplied to a function, rather than their values. For example, note the
    axis labels in this plot:

    ```{r, fig.width = 4, fig.height = 2.5}
    par(mar = c(4.5, 4.5, 1, 0.5))
    grid <- seq(0, 2 * pi, length = 100)
    plot(grid, sin(grid), type = "l")
    ```

1.  __Non-standard scoping__ looks for objects in places other than the current
    environment. For example, base R provides `with()`, `subset()`, and 
    `transform()` that look for names in a data frame (or list) before the 
    current environment:

    ```{r}
    df <- data.frame(x = c(1, 5, 4, 2, 3), y = c(2, 1, 5, 4, 3))
    
    with(df, mean(x))
    subset(df, x == y)
    transform(df, z = x + y)
    ```

1.  __Metaprogramming__ is a catch-all term that covers all other uses of 
    NSE (such as in `bquote()` and `library()`). Metaprogramming is so called 
    because it involves computing on the unevaluated code in some way.

This document is broadly organised according to the three types of non-standard evaluation described above. The main difference is that after [labelling], we'll take a detour to learn more about [formulas]. You've probably familiar with formulas from linear models (e.g. `lm(mpg ~ displ, data = mtcars)`), but formuals are actually a general way of capturing an unevaluated expression along with the environment in which it should be evaluated. 

My recommendations for how to do this have changed substantially over time. I am fairly confident they will not have to change again. This approach and accompanying tools allows you to solve a wide range of practical problems that were challenging previously and is rooted in [well-standing theory](http://repository.readscheme.org/ftp/papers/pepm99/bawden.pdf).

[^1]: Currently neither ggplot2 nor dplyr actually use this technique since I've only just figured it out. But I'll be working hard to make sure all my packages are consistent in the near future.

## Labelling

In base R, the classic way to turn an argument into a label is to use `deparse(substitute(x))`:

```{r}
my_label <- function(x) deparse(substitute(x))
my_label(x + y)
```

There are two potential problems with this approach:

1.  For long some expressions, `deparse()` generates a character vector with 
    length > 1:
    
    ```{r}
    my_label({
      a + b
      c + d
    })
    ```

1.  `substitute()` only looks one level up, so you lose the original label if 
    the function isn't called from the top level:
    
    ```{r}
    my_label2 <- function(x) my_label(x)
    my_label2(a + b)
    ```

Both of these problems are resolved by `lazyeval::expr_text()`:

```{r}
my_label <- function(x) expr_text(x)
my_label2 <- function(x) my_label(x)
   
my_label({
  a + b
  c + d
})
my_label2(a + b)
```

There are two variations on the theme of `expr_text()`:

*   `expr_find()` find the underlying expression. It works similarly to 
    `substitute()` but will follow a chain of promises back up to the original
    expression. This is an important component of [metaprogramming].
  
*   `expr_label()` is a customised version of `expr_text()` that produces 
    labels designed to be used in messages to the user:

    ```{r}
    expr_label(x)
    expr_label(a + b + c)
    expr_label(foo({
      x + y
    }))
    ```

### Exercises

1.  `plot()` uses `deparse(substitute(x))` to generate labels for the x and y
    axes. Can you generate input that causes it to display bad labels?
    Write your own wrapper around `plot()` that uses `expr_label()` to compute
    `xlim` and `ylim`.
    
1.  Read the source code for `expr_text()`. How does it work? What additional
    arguments to `deparse()` does it use?

## Formulas

Before we can talk about non-standard scoping, we need to take a detour to talk about formulas. Formulas are a familiar tool from linear models, but they are actually a powerful, general purpose tool. A formula captures two things: 

1. An unevaluated expression.
1. The environment in which the expression was created.

`~` is a single character that allows you to say: "I want to capture the meaning of this code, without evaluating it right away". For that reason, the formula can be thought of as general "quoting" operator.

### Definition of a formula

Technically, a formula is a "language" object (i.e. an unevaluated expression) with an S3 class of "formula" and an attribute that stores the environment:

```{r}
f <- ~ x + y + z
typeof(f)
attributes(f)
```

The structure of the underlying object is slightly different depending on whether you have a one-sided or two-sided formula:

*   One-sided formulas have length two:

    ```{r}
    length(f)
    # The 1st element is always ~
    f[[1]]
    # The 2nd element is the quoted expression
    f[[2]]
    ```

*   Two-sided formulas have length three:

    ```{r}
    g <- y ~ x + z
    length(g)
    # The 1st element is still ~
    g[[1]]
    # But now the 2nd element is the LHS
    g[[2]]
    # And the 3rd element is the RHS
    g[[3]]
    ```

To abstract away these differences, lazyeval provides `f_rhs()` and `f_lhs()` to access either side of the formula, and `f_env()` to access the environment:

```{r}
f_rhs(f)
f_lhs(f)
f_env(f)

f_rhs(g)
f_lhs(g)
f_env(g)
```

### Evaluating a formula

A formula captures the meaning of an expression without evaluating it. To force evaluation, use `f_eval()`:

```{r}
f <- ~ 1 + 2 + 3
f
f_eval(f)
```

This allows you to use a formula as a robust way of delaying evaluation. This works even when the formula is used in a different place from where it was defined. In the following example, note that the value of `x` inside `add_1000()` is used, not the value of `x` at the top level.

```{r}
x <- 1
add_1000 <- function(x) {
  ~ 1000 + x
}

add_1000(3)
f_eval(add_1000(3))
```

It can be hard to see what's going on when looking at a formula because important values are stored in the environment, which is largely opaque. You can use `f_unwrap()` to replace names with their corresponding values:

```{r}
f_unwrap(add_1000(3))
```

### Non-standard scoping

`f_eval()` has an optional second argument: a named list (or data frame) that overrides values found in the formula's environment. This makes it very easy to implement non-standard scoping:

```{r}
y <- 100
f_eval(~ x + y, data = list(x = 10))
f_eval(~ f(y), data = list(f = function(x) x * 3))

f_eval(~ mean(cyl), data = mtcars)
```

One challenge with non-standard scoping is that we've introduced some ambiguity. For example, in the code below, does `x` come from `mydata` or the environment?

```{r, eval = FALSE}
f_eval(~ x, data = mydata)
```

You can't know without knowing whether or not `mydata` has a variable called `x`. To overcome this problem, `f_eval` provides two pronouns:

* `.data` is bound to the data frame.
* `.env` is bound to the formula environment.

They both start with `.` to minimise the chances of clashing with existing variables.

With these pronouns we can rewrite the previous formula to remove the ambiguity:

```{r}
mydata <- data.frame(x = 100, y = 1)
x <- 10

f_eval(~ .env$x, data = mydata)
f_eval(~ .data$x, data = mydata)
```

If the variable or object doesn't exist, you'll get an informative error:

```{r, error = TRUE}
f_eval(~ .env$z, data = mydata)
f_eval(~ .data$z, data = mydata)
```

### Unquoting

`f_eval()` has one more useful trick up its sleeve: unquoting. This allows you to write functions where the user supplies part of the formula. For example, the following function allows you to compute the mean of any column (or any function of a column):

```{r}
df_mean <- function(df, variable) {
  f_eval(~ mean(uq(variable)), data = df)
}

df_mean(mtcars, ~ cyl)
df_mean(mtcars, ~ disp * 0.01638)
df_mean(mtcars, ~ sqrt(mpg))
```

To see how this works, we can use `f_interp()` which `f_eval()` calls internally (you shouldn't call it in your own code, but it's useful for debugging). 

`uq()` evaluates it's first argument and inserts its value into the formula:
    
```{r}
variable <- ~cyl
f_interp(~ mean(uq(variable)))

variable <- ~ disp * 0.01638
f_interp(~ mean(uq(variable)))
```

Unquoting is powerful, but it only allows you to modify a specific argument: it doesn't allow you to add an arbitrary number of arguments. To do that, you'll need "unquote-splice", or `uqs()`. The first argument to `uqs()` should be a list of arguments to be spliced into the call:

```{r}
variable <- ~ x
extra_args <- list(na.rm = TRUE, trim = 0.9)
f_interp(~ mean(uq(variable), uqs(extra_args)))
```

Note that you can also use `uq()` to change the function being called:

```{r}
f <- ~ mean
f_interp(~ uq(f)(x))
```

Note that `uq()` only takes the RHS of a formula, which makes it difficult to insert literal formulas into a call:

```{r}
formula <- y ~ x
f_interp(~ lm(uq(formula), data = df))
```

You can instead use `uqf()` which uses the whole formula, not just the RHS:

```{r}
f_interp(~ lm(uqf(formula), data = df))
```

### Exercises

1.  Compare and contrast `f_eval()` with `with()`.

## Non-standard scoping

Non-standard scoping (NSS) is an important part of R because it makes it makes it possible to write functions that are tailored for interactive data exploration. These functions require less typing, at the expense of introducing some ambiguity and "magic". This is a good trade-off for interactive data exploration because you want to get ideas out of your head and into the computer as quickly as possible. If a function does make a bad guess, you'll spot it quickly because you're working interactively.

There are three challenges to implementing non-standard scoping:

1.  You must correctly delay the evaluation of a function argument, capturing 
    both the computation (the expression), and the context (the environment).
    I recommend making this explicit by requiring the user to "quote" any NSS
    arguments with `~`.
  
1.  When wrapping an NSS-function in another function, you need some way to
    avoid the automatic lookup and be explicit about where objects should be
    found.

1.  You need to be to able to mix fix and varying components so that the user
    can specify some components of the formula with arguments to the function.

To illustrate these challenges, I will implement a `sieve()` function that works similarly to `base::subset()` or `dplyr::filter()`. The goal of `sieve()` is to make it easy to select observations that match criteria defined by variable values. `sieve()` has three advantages over `[`:

1.  It is much more compact when the condition uses many variables, because 
    you don't need to repeat the name of the data frame many times.

1.  It drops rows where the condition evaluates to `NA`, rather than filling 
    them with `NA`s.
    
1.  It always returns a data frame.

The implementation of `sieve()` is straightforward. First we use `f_eval()` to perform NSS. Then we then check that we have a logical vector, replace `NA`s with `FALSE`, and subset with `[`.

```{R}
sieve <- function(df, condition) {
  rows <- f_eval(condition, df)
  if (!is.logical(rows)) {
    stop("`condition` must be logical.", call. = FALSE)
  }
  
  rows[is.na(rows)] <- FALSE
  df[rows, , drop = FALSE]
}

df <- data.frame(x = 1:5, y = 5:1)
sieve(df, ~ x <= 2)
sieve(df, ~ x == y)
```

### Programming with `sieve()`

Imagine that you've written some code that looks like this:

```{r, eval = FALSE}
sieve(march, ~ x > 100)
sieve(april, ~ x > 50)
sieve(june, ~ x > 45)
sieve(july, ~ x > 17)
```

(This is admittedly a contrived example, but it illustrates all of the important issues you'll need to consider when writing more useful functions.)

Instead of continuing to copy-and-paste your code, you decide to wrap up the common behaviour in a function: 

```{r}
threshold_x <- function(df, threshold) {
  sieve(df, ~ x > threshold)
}
threshold_x(df, 3)
```

There are two ways that this function might fail:

1.  The data frame might not have a variable called `x`. This will fail unless
    there's a variable called `x` hanging around in the global environment:
    
    ```{r, error = TRUE}
    rm(x)
    df2 <- data.frame(y = 5:1)
    
    # Throws an error
    threshold_x(df2, 3)
    
    # Silently gives the incorrect result!
    x <- 5
    threshold_x(df2, 3)
    ```
    
1.  The data frame might have a variable called `threshold`:

    ```{r}
    df3 <- data.frame(x = 1:5, y = 5:1, threshold = 4)
    threshold_x(df3, 3)
    ```

These failures are partiuclarly pernicious because they don't throw an error, but silently give an incorrect result. Both failures arise because `f_eval()` looks in two places for each name: the data frame and formula environment. To make this function more reliable, we need to be more explicit by using the `.data` and `.env` pronouns:

```{r, error = TRUE}
threshold_x <- function(df, threshold) {
  sieve(df, ~ .data$x > .env$threshold)
}

threshold_x(df2, 3)
threshold_x(df3, 3)
```

Here `.env` is bound to the environment where `~` is evaluated, namely the inside of `threshold_x()`.

### Adding argument

The `threshold_x()` function is not very useful because it's bound to a specific variable. It would be more powerful if we could vary the variable as well as the threshold. We can do that by taking an additional argument to specify which variable to use. One simple approach is to use a string and `[[`:

```{r}
threshold <- function(df, variable, threshold) {
  stopifnot(is.character(variable), length(variable) == 1)
  
  sieve(df, ~ .data[[.env$variable]] > .env$threshold)
}
threshold(df, "x", 4)
```

This is a simple and robust solution, but only allows us to use an existing variable, not an arbitrary expression like `sqrt(x)`.

A more general solution is to allow the user to supply a formula, and use unquoting:

```{r}
threshold <- function(df, variable = ~x, threshold = 0) {
  sieve(df, ~ uq(variable) > .env$threshold)
}

threshold(df, ~ x, 4)
threshold(df, ~ abs(x - y), 2)
```

It's not your responsibility to make `variable` robust. If the user wants to distinguish between variables in the data frame and in the environment, they can:

```{r}
x <- 3
threshold(df, ~ .data$x - .env$x, 0)
```

### Eliminating the formulas

In some situations you might want to eliminate the formula altogether, and allow the user to type expressions directly. Although I was once much enamoured with this approach (witness ggplot2, dplyr, ...), I now think that it should be used sparingly. Requiring explict quoting with `~`, leads to simpler code, and the explict quoting makes it clear to the user that something special will happen with the argument.

That said, lazyeval does allow you to eliminate the `~` if you really want to. You'll still need a function that works with a formula, so give it the suffix `_`:

```{r}
sieve_ <- function(df, condition) {
  rows <- f_eval(condition, df)
  if (!is.logical(rows)) {
    stop("`condition` must be logical.", call. = FALSE)
  }
  
  rows[is.na(rows)] <- FALSE
  df[rows, , drop = FALSE]
}
```

Once you have this version you can create a verison that doesn't need the explicit formula. The key is the use of `f_capture()` which takes an unevaluated argument and captures it as a formula:

```{r}
sieve <- function(df, expr) {
  sieve_(df, f_capture(expr))
}
sieve(df, x == 1)
```

If you're familiar with `substitute()` you might expect the same drawbacks to apply. However, `f_capture()` is smart enough to follow a chain of promises back to the original value, so, for example, this code works fine:

```{r}
scramble <- function(df) {
  df[sample(nrow(df)), , drop = FALSE]
}
subscramble <- function(df, expr) {
  scramble(sieve(df, expr))
}
subscramble(df, x < 4)
```

### Dot-dot-dot

There is one additional tool that you might find useful for functions that take `...`. For example, take this lazyeval implementation of `dplyr::mutate()`. You give it a data frame, and set of named expresssion:

```{r}
mutate <- function(`_df`, ...) {
  args <- list(...)
  
  for (nm in names(args)) {
    `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  }
  
  `_df`
}

df <- data.frame(x = 1:5, y = sample(5))
mutate(df, z = ~ x + y, z2 = ~ z * 2)
```

(NB: the first argument is a non-syntactic name (i.e. it requires quoting with `` ` ``) so it doesn't accidentally match one of the names of the new variables. If this doesn't make sense, modify the above code to use `x` instead of `` `_df` `` and see what happens when you try and create a new variable called `x`.)

One that that's a little awkward with this formulation is changing the names of the generated variables. Lazyeval provides the `f_list()` function to make this a little easier. It takes a list of formulas and evaluates the LHS to determine the name of the element:

```{r}
f_list("x" ~ y, z = ~z)
```

If we tweak `mutate()` to use `f_list()` instead of `list()`:

```{r}
mutate <- function(`_df`, ...) {
  args <- f_list(...)
  
  for (nm in names(args)) {
    `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  }
  
  `_df`
}
```

We can then create a function that allows us to double a variable:

```{r}
double <- function(df, var) {
  if (!is_formula(var)) {
    stop("`var` must be a formula", call. = FALSE)
  }
  var_name <- f_rhs(var)
  if (!is_name(var_name)) {
    stop("`var` must be of the form `~ var`", call. =
        FALSE)
  }

  mutate(df, paste0(var_name, "2") ~ uq(var) * 2)
}
double(df, ~ x)
```

If you want a version that doesn't require formulas, I recommend that the SE version take a list as an argument. Use `dots_capture()` to capture multiple arguments as a list of formulas:

```{r}
mutate_ <- function(`_df`, args) {
  args <- as_f_list(args)
  
  for (nm in names(args)) {
    `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  }
  
  `_df`
}

mutate <- function(`_df`, ...) {
  mutate_(`_df`, dots_capture(...))
}
```

### Exercises

1.  Write a function that selects all rows of `df` where `variable` is 
    greater than its mean.

1.  Recreate `subscramble()` using `base::subset()` instead of `sieve()`.
    Why does it fail?

## Metaprogramming
